"""
Advanced Time Series Forecasting with Attention-LSTM

Requirements:
    pip install numpy pandas matplotlib scikit-learn tensorflow
(Optionally statsmodels if you want SARIMA, but here we use an MLP baseline.)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

import tensorflow as tf
from tensorflow.keras import layers, Model, Input


# -------------------------------------------------------------------------
# 1. Reproducibility
# -------------------------------------------------------------------------
np.random.seed(42)
tf.random.set_seed(42)


# -------------------------------------------------------------------------
# 2. Data Generation: Multivariate Time Series with Seasonality + Trend
# -------------------------------------------------------------------------
def generate_synthetic_multivariate_ts(n_steps=5000, freq="H"):
    """
    Generate a synthetic multivariate time series with:
    - clear seasonality (daily & weekly)
    - trend
    - noise
    - multiple features + one target

    Returns:
        df (pd.DataFrame): columns [feat1, feat2, feat3, target]
    """
    # Time index
    time_index = pd.date_range(start="2020-01-01", periods=n_steps, freq=freq)

    t = np.arange(n_steps)

    # Seasonal components
    daily_period = 24
    weekly_period = 24 * 7

    daily_seasonality = 10 * np.sin(2 * np.pi * t / daily_period)
    weekly_seasonality = 5 * np.sin(2 * np.pi * t / weekly_period)

    # Trend (non-stationarity)
    trend = 0.01 * t

    # Feature 1: strong daily seasonality + trend + noise
    feat1 = daily_seasonality + trend + np.random.normal(scale=2.0, size=n_steps)

    # Feature 2: weekly seasonality + noise
    feat2 = weekly_seasonality + np.random.normal(scale=1.5, size=n_steps)

    # Feature 3: random walk / low-frequency drift
    random_walk = np.cumsum(np.random.normal(scale=0.1, size=n_steps))
    feat3 = random_walk + np.random.normal(scale=0.5, size=n_steps)

    # Target: nonlinear combination of features + noise
    target = (
        0.5 * feat1
        + 0.3 * feat2
        + 0.2 * feat3
        + 0.1 * np.sin(2 * np.pi * t / (daily_period * 2))
        + np.random.normal(scale=2.0, size=n_steps)
    )

    df = pd.DataFrame(
        {
            "feat1": feat1,
            "feat2": feat2,
            "feat3": feat3,
            "target": target,
        },
        index=time_index,
    )

    return df


# -------------------------------------------------------------------------
# 3. Preprocessing: Scaling + Windowing
# -------------------------------------------------------------------------
def create_windows(data, target_col, lookback=48, horizon=1):
    """
    Turn a multivariate time series into supervised learning windows.

    Args:
        data (np.ndarray): scaled data of shape (n_steps, n_features)
        target_col (int): index of the target column in data
        lookback (int): number of timesteps in the input sequence
        horizon (int): prediction horizon (timesteps ahead, here 1)

    Returns:
        X: (n_samples, lookback, n_features)
        y: (n_samples, 1)
    """
    X, y = [], []
    n_steps = data.shape[0]
    for i in range(lookback, n_steps - horizon + 1):
        X.append(data[i - lookback : i, :])
        # Predict target at time i + horizon - 1
        y.append(data[i + horizon - 1, target_col])
    return np.array(X), np.array(y).reshape(-1, 1)


# -------------------------------------------------------------------------
# 4. Baseline Model: Simple MLP
# -------------------------------------------------------------------------
def build_mlp_baseline(input_shape):
    """
    Baseline model: flatten the window and feed it into a simple MLP.
    """
    model = tf.keras.Sequential(
        [
            Input(shape=input_shape),
            layers.Flatten(),
            layers.Dense(64, activation="relu"),
            layers.Dense(32, activation="relu"),
            layers.Dense(1),
        ]
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="mse",
        metrics=[tf.keras.metrics.MeanAbsoluteError(name="mae")],
    )
    return model


# -------------------------------------------------------------------------
# 5. Attention Layer (Bahdanau-style over time dimension)
# -------------------------------------------------------------------------
class BahdanauAttention(layers.Layer):
    """
    Bahdanau-style attention over the time dimension.

    Input:  encoder_out -> shape (batch_size, timesteps, hidden_dim)
    Output: context_vector -> (batch_size, hidden_dim)
            attention_weights -> (batch_size, timesteps, 1)
    """

    def __init__(self, units, **kwargs):
        super(BahdanauAttention, self).__init__(**kwargs)
        self.units = units

    def build(self, input_shape):
        hidden_dim = input_shape[-1]
        # W1: (hidden_dim, units)
        self.W1 = self.add_weight(
            name="W1",
            shape=(hidden_dim, self.units),
            initializer="glorot_uniform",
            trainable=True,
        )
        # W2: (hidden_dim, units) - for decoder state, but we'll simplify and
        #      treat decoder state as a learned biasless parameter for now.
        self.W2 = self.add_weight(
            name="W2",
            shape=(hidden_dim, self.units),
            initializer="glorot_uniform",
            trainable=True,
        )
        # v: (units, 1)
        self.V = self.add_weight(
            name="V",
            shape=(self.units, 1),
            initializer="glorot_uniform",
            trainable=True,
        )
        super(BahdanauAttention, self).build(input_shape)

    def call(self, encoder_outputs):
        """
        encoder_outputs: (batch_size, timesteps, hidden_dim)
        """
        # We're not using an explicit decoder hidden state here; we
        # approximate with a learnable transformation of encoder_outputs.
        # Compute score e_ij for each timestep j:
        # score = v^T * tanh(W1*h_j + W2*mean(h))
        # where mean(h) acts like a global context / pseudo decoder state.
        h = encoder_outputs
        # mean over time -> (batch_size, hidden_dim)
        h_mean = tf.reduce_mean(h, axis=1)

        # (batch_size, timesteps, units)
        score_part1 = tf.tensordot(h, self.W1, axes=[[2], [0]])
        # expand mean to timesteps
        h_mean_expanded = tf.expand_dims(h_mean, axis=1)
        score_part2 = tf.tensordot(h_mean_expanded, self.W2, axes=[[2], [0]])

        score = tf.nn.tanh(score_part1 + score_part2)

        # (batch_size, timesteps, 1)
        score = tf.tensordot(score, self.V, axes=[[2], [0]])

        # Attention weights (softmax over timesteps)
        attention_weights = tf.nn.softmax(score, axis=1)

        # Context vector as weighted sum of encoder_outputs
        context_vector = attention_weights * h  # broadcasting
        context_vector = tf.reduce_sum(context_vector, axis=1)  # (batch, hidden_dim)

        return context_vector, attention_weights


# -------------------------------------------------------------------------
# 6. Attention-based LSTM Model
# -------------------------------------------------------------------------
def build_attention_lstm(input_shape, lstm_units=64, attention_units=32):
    """
    Build an LSTM model with Bahdanau-style attention on top.
    Attention is over timesteps of the LSTM hidden states.
    """
    inputs = Input(shape=input_shape)  # (lookback, n_features)

    # LSTM encoder
    x = layers.LSTM(lstm_units, return_sequences=True)(inputs)  # (batch, T, H)

    # Attention
    context_vector, attention_weights = BahdanauAttention(attention_units)(x)

    # Final regression head
    x = layers.Dense(64, activation="relu")(context_vector)
    outputs = layers.Dense(1)(x)

    model = Model(inputs=inputs, outputs=outputs, name="Attention_LSTM")

    # Model to extract attention weights later
    attention_model = Model(inputs=inputs, outputs=attention_weights, name="Attention_Extractor")

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="mse",
        metrics=[tf.keras.metrics.MeanAbsoluteError(name="mae")],
    )

    return model, attention_model


# -------------------------------------------------------------------------
# 7. Training / Evaluation Utilities
# -------------------------------------------------------------------------
def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, epochs=20, batch_size=64):
    """
    Train a Keras model and return history + predictions + metrics.
    """
    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1,
    )

    y_pred_test = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    return history, y_pred_test, mae, rmse


# -------------------------------------------------------------------------
# 8. Plotting Helpers
# -------------------------------------------------------------------------
def plot_predictions(y_true, y_pred, title="Predictions vs Actual", n_points=200):
    """
    Plot a slice of true vs predicted values.
    """
    plt.figure(figsize=(10, 4))
    plt.plot(y_true[:n_points], label="Actual")
    plt.plot(y_pred[:n_points], label="Predicted")
    plt.title(title)
    plt.xlabel("Time step")
    plt.ylabel("Scaled target")
    plt.legend()
    plt.tight_layout()
    plt.show()


def plot_attention_weights(attention_weights, lookback, sample_index=0, title="Attention over Time"):
    """
    Plot attention weights for a given sample index.
    attention_weights shape: (batch_size, timesteps, 1)
    """
    aw = attention_weights[sample_index, :, 0]  # (timesteps,)
    timesteps = np.arange(-lookback, 0)  # relative time (past steps)

    plt.figure(figsize=(10, 4))
    plt.stem(timesteps, aw, use_line_collection=True)
    plt.title(title + f" (sample {sample_index})")
    plt.xlabel("Time steps relative to prediction (negative = past)")
    plt.ylabel("Attention weight")
    plt.tight_layout()
    plt.show()


# -------------------------------------------------------------------------
# 9. Main Pipeline
# -------------------------------------------------------------------------
def main():
    # -------------------------------
    # 9.1 Generate dataset
    # -------------------------------
    df = generate_synthetic_multivariate_ts(n_steps=5000, freq="H")
    print("Dataset head:")
    print(df.head())
    print("\nDataset description:")
    print(df.describe())

    # -------------------------------
    # 9.2 Train/Val/Test split
    # -------------------------------
    # 70% train, 15% val, 15% test
    n = len(df)
    train_end = int(0.7 * n)
    val_end = int(0.85 * n)

    train_df = df.iloc[:train_end]
    val_df = df.iloc[train_end:val_end]
    test_df = df.iloc[val_end:]

    # -------------------------------
    # 9.3 Scaling
    # -------------------------------
    feature_cols = ["feat1", "feat2", "feat3", "target"]
    target_col_name = "target"
    target_col_index = feature_cols.index(target_col_name)

    scaler = StandardScaler()
    scaler.fit(train_df[feature_cols])

    train_scaled = scaler.transform(train_df[feature_cols])
    val_scaled = scaler.transform(val_df[feature_cols])
    test_scaled = scaler.transform(test_df[feature_cols])

    # -------------------------------
    # 9.4 Windowing
    # -------------------------------
    lookback = 48   # past 48 time steps (e.g., last 2 days if hourly)
    horizon = 1     # 1-step ahead forecasting

    X_train, y_train = create_windows(train_scaled, target_col_index, lookback, horizon)
    X_val, y_val = create_windows(val_scaled, target_col_index, lookback, horizon)
    X_test, y_test = create_windows(test_scaled, target_col_index, lookback, horizon)

    print(f"\nWindowed shapes:")
    print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
    print(f"X_val:   {X_val.shape}, y_val:   {y_val.shape}")
    print(f"X_test:  {X_test.shape}, y_test:  {y_test.shape}")

    input_shape = X_train.shape[1:]  # (lookback, n_features)

    # -------------------------------
    # 9.5 Baseline model (MLP)
    # -------------------------------
    print("\n--- Training Baseline MLP ---")
    mlp_model = build_mlp_baseline(input_shape)
    mlp_model.summary()

    mlp_history, mlp_y_pred_test, mlp_mae, mlp_rmse = evaluate_model(
        mlp_model,
        X_train,
        y_train,
        X_val,
        y_val,
        X_test,
        y_test,
        epochs=15,
        batch_size=64,
    )

    print(f"\nBaseline MLP Test MAE:  {mlp_mae:.4f}")
    print(f"Baseline MLP Test RMSE: {mlp_rmse:.4f}")

    plot_predictions(
        y_test,
        mlp_y_pred_test,
        title="Baseline MLP: Predictions vs Actual (Test set)",
        n_points=200,
    )

    # -------------------------------
    # 9.6 Attention-LSTM model
    # -------------------------------
    print("\n--- Training Attention-LSTM ---")
    att_lstm_model, attention_extractor = build_attention_lstm(
        input_shape, lstm_units=64, attention_units=32
    )
    att_lstm_model.summary()

    att_history, att_y_pred_test, att_mae, att_rmse = evaluate_model(
        att_lstm_model,
        X_train,
        y_train,
        X_val,
        y_val,
        X_test,
        y_test,
        epochs=20,
        batch_size=64,
    )

    print(f"\nAttention-LSTM Test MAE:  {att_mae:.4f}")
    print(f"Attention-LSTM Test RMSE: {att_rmse:.4f}")

    plot_predictions(
        y_test,
        att_y_pred_test,
        title="Attention-LSTM: Predictions vs Actual (Test set)",
        n_points=200,
    )

    # -------------------------------
    # 9.7 Compare performance
    # -------------------------------
    print("\n--- Comparative Performance ---")
    print(f"Baseline MLP   -> MAE: {mlp_mae:.4f}, RMSE: {mlp_rmse:.4f}")
    print(f"Attention-LSTM -> MAE: {att_mae:.4f}, RMSE: {att_rmse:.4f}")

    # -------------------------------
    # 9.8 Attention analysis
    # -------------------------------
    print("\n--- Extracting Attention Weights ---")
    # Get attention weights for some subset of test samples
    # Use the same X_test we used for predictions
    att_weights_test = attention_extractor.predict(X_test)  # shape (batch, T, 1)

    print(f"Attention weights shape: {att_weights_test.shape}")

    # Plot attention weights for one sample
    sample_idx = 0
    plot_attention_weights(
        att_weights_test,
        lookback=lookback,
        sample_index=sample_idx,
        title="Attention over Past Timesteps (Attention-LSTM)",
    )

    # You can also inspect which timesteps get the highest attention:
    top_k = 5
    aw = att_weights_test[sample_idx, :, 0]
    top_indices = np.argsort(aw)[-top_k:][::-1]
    print(f"\nTop {top_k} most attended timesteps (0 = farthest past, {lookback-1} = most recent):")
    for idx in top_indices:
        print(f"  timestep {idx - lookback} (relative), attention = {aw[idx]:.4f}")


if __name__ == "__main__":
    main()
