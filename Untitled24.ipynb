{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dp-4RoWttSV",
        "outputId": "2e356f0a-5e25-4134-e16b-deff4f3c5584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Layer, Multiply, Add\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesGenerator:\n",
        "    \"\"\"Generate synthetic multivariate time series data with seasonality and noise\"\"\"\n",
        "\n",
        "    def __init__(self, n_samples=2000, n_features=4, seasonal_periods=24, noise_level=0.2):\n",
        "        self.n_samples = n_samples\n",
        "        self.n_features = n_features\n",
        "        self.seasonal_periods = seasonal_periods\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\"Generate multivariate time series with seasonality and trends\"\"\"\n",
        "        time = np.arange(self.n_samples)\n",
        "\n",
        "        features = []\n",
        "\n",
        "        # Feature 1: Strong seasonal pattern with trend\n",
        "        seasonal_1 = 2 * np.sin(2 * np.pi * time / self.seasonal_periods)\n",
        "        trend_1 = 0.01 * time\n",
        "        noise_1 = np.random.normal(0, self.noise_level, self.n_samples)\n",
        "        feature_1 = seasonal_1 + trend_1 + noise_1\n",
        "        features.append(feature_1)\n",
        "\n",
        "        # Feature 2: Different seasonal pattern\n",
        "        seasonal_2 = 1.5 * np.sin(2 * np.pi * time / (self.seasonal_periods/2) + np.pi/4)\n",
        "        trend_2 = 0.005 * time - 0.0001 * time**2\n",
        "        noise_2 = np.random.normal(0, self.noise_level, self.n_samples)\n",
        "        feature_2 = seasonal_2 + trend_2 + noise_2\n",
        "        features.append(feature_2)\n",
        "\n",
        "        # Feature 3: Weekly seasonality\n",
        "        seasonal_3 = np.sin(2 * np.pi * time / (7 * self.seasonal_periods))\n",
        "        trend_3 = 0.008 * time\n",
        "        noise_3 = np.random.normal(0, self.noise_level * 1.5, self.n_samples)\n",
        "        feature_3 = seasonal_3 + trend_3 + noise_3\n",
        "        features.append(feature_3)\n",
        "\n",
        "        # Feature 4: Random walk component\n",
        "        random_walk = np.cumsum(np.random.normal(0, 0.1, self.n_samples))\n",
        "        seasonal_4 = 0.5 * np.sin(2 * np.pi * time / self.seasonal_periods * 3)\n",
        "        feature_4 = random_walk + seasonal_4\n",
        "        features.append(feature_4)\n",
        "\n",
        "        # Create DataFrame\n",
        "        data = np.column_stack(features)\n",
        "        columns = [f'feature_{i+1}' for i in range(self.n_features)]\n",
        "        self.df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "        # Add timestamp index\n",
        "        dates = pd.date_range(start='2020-01-01', periods=self.n_samples, freq='H')\n",
        "        self.df.index = dates\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def create_target(self):\n",
        "        \"\"\"Create target variable as a combination of features with lag effects\"\"\"\n",
        "        # Target depends on current and lagged values of features\n",
        "        target = (0.4 * self.df['feature_1'] +\n",
        "                  0.3 * self.df['feature_2'].shift(1).fillna(0) +\n",
        "                  0.2 * self.df['feature_3'].shift(2).fillna(0) +\n",
        "                  0.1 * self.df['feature_4'] +\n",
        "                  np.random.normal(0, 0.1, len(self.df)))\n",
        "\n",
        "        self.df['target'] = target\n",
        "        return self.df\n",
        "\n",
        "# Generate the dataset\n",
        "print(\"Generating synthetic time series data...\")\n",
        "ts_generator = TimeSeriesGenerator(n_samples=2000, n_features=4, seasonal_periods=24, noise_level=0.15)\n",
        "df = ts_generator.generate_data()\n",
        "df = ts_generator.create_target()\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nDataset head:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset info:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F8VQogyt87s",
        "outputId": "d9f3fb9b-7c03-45cd-caad-570695a85b3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic time series data...\n",
            "Dataset shape: (2000, 5)\n",
            "\n",
            "Dataset head:\n",
            "                     feature_1  feature_2  feature_3  feature_4    target\n",
            "2020-01-01 00:00:00   0.074507   0.959383  -0.194286  -0.111408  0.015360\n",
            "2020-01-01 01:00:00   0.506898   1.432111   0.038370   0.179052  0.458115\n",
            "2020-01-01 02:00:00   1.117153   1.339626   0.094784   0.231293  0.843529\n",
            "2020-01-01 03:00:00   1.672668   1.028566   0.242306   0.030047  1.153107\n",
            "2020-01-01 04:00:00   1.736928   0.122586  -0.126501  -0.344922  1.115591\n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 2000 entries, 2020-01-01 00:00:00 to 2020-03-24 07:00:00\n",
            "Freq: h\n",
            "Data columns (total 5 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   feature_1  2000 non-null   float64\n",
            " 1   feature_2  2000 non-null   float64\n",
            " 2   feature_3  2000 non-null   float64\n",
            " 3   feature_4  2000 non-null   float64\n",
            " 4   target     2000 non-null   float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 93.8 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "    \"\"\"Handle data preprocessing for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, lookback=48, forecast_horizon=12, train_ratio=0.7, val_ratio=0.15):\n",
        "        self.lookback = lookback\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.train_ratio = train_ratio\n",
        "        self.val_ratio = val_ratio\n",
        "        self.scaler_x = StandardScaler()\n",
        "        self.scaler_y = StandardScaler()\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"Prepare data for training and testing\"\"\"\n",
        "        # Separate features and target\n",
        "        X = df[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
        "        y = df['target'].values.reshape(-1, 1)\n",
        "\n",
        "        # Scale the data\n",
        "        X_scaled = self.scaler_x.fit_transform(X)\n",
        "        y_scaled = self.scaler_y.fit_transform(y)\n",
        "\n",
        "        # Create sequences\n",
        "        X_seq, y_seq = self.create_sequences(X_scaled, y_scaled)\n",
        "\n",
        "        # Split the data\n",
        "        return self.train_val_test_split(X_seq, y_seq)\n",
        "\n",
        "    def create_sequences(self, X, y):\n",
        "        \"\"\"Create input sequences and corresponding targets\"\"\"\n",
        "        X_seq, y_seq = [], []\n",
        "\n",
        "        for i in range(len(X) - self.lookback - self.forecast_horizon + 1):\n",
        "            X_seq.append(X[i:(i + self.lookback)])\n",
        "            y_seq.append(y[i + self.lookback:i + self.lookback + self.forecast_horizon])\n",
        "\n",
        "        return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "    def train_val_test_split(self, X, y):\n",
        "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
        "        n_samples = len(X)\n",
        "        train_end = int(n_samples * self.train_ratio)\n",
        "        val_end = train_end + int(n_samples * self.val_ratio)\n",
        "\n",
        "        X_train, y_train = X[:train_end], y[:train_end]\n",
        "        X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
        "        X_test, y_test = X[val_end:], y[val_end:]\n",
        "\n",
        "        print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "        print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "        print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
        "\n",
        "        return (X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "# Preprocess the data\n",
        "print(\"\\nPreprocessing data...\")\n",
        "preprocessor = DataPreprocessor(lookback=48, forecast_horizon=12)\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocessor.prepare_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hAOp5vRuBV-",
        "outputId": "126336c9-455e-42ea-d877-6e4a7d05e207"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing data...\n",
            "Training set: (1358, 48, 4), (1358, 12, 1)\n",
            "Validation set: (291, 48, 4), (291, 12, 1)\n",
            "Test set: (292, 48, 4), (292, 12, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "class BaselineModels:\n",
        "    \"\"\"Implement baseline models for performance comparison\"\"\"\n",
        "\n",
        "    def __init__(self, preprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def sarima_baseline(self, y_train, y_test):\n",
        "        \"\"\"SARIMA baseline model\"\"\"\n",
        "        print(\"Training SARIMA baseline...\")\n",
        "\n",
        "        # Use only the target variable for SARIMA\n",
        "        y_train_original = self.preprocessor.scaler_y.inverse_transform(\n",
        "            y_train[:, -1].reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Fit SARIMA model (simplified for demonstration)\n",
        "        try:\n",
        "            model = SARIMAX(y_train_original, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))\n",
        "            sarima_fit = model.fit(disp=False)\n",
        "\n",
        "            # Forecast\n",
        "            forecast = sarima_fit.forecast(steps=len(y_test))\n",
        "\n",
        "            # Calculate metrics\n",
        "            y_test_original = self.preprocessor.scaler_y.inverse_transform(\n",
        "                y_test[:, -1].reshape(-1, 1)).flatten()\n",
        "\n",
        "            mae = mean_absolute_error(y_test_original, forecast)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test_original, forecast))\n",
        "\n",
        "            print(f\"SARIMA - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "            return forecast, mae, rmse\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"SARIMA failed: {e}\")\n",
        "            # Return naive forecast as fallback\n",
        "            last_value = y_train_original[-1]\n",
        "            forecast = np.full(len(y_test), last_value)\n",
        "\n",
        "            y_test_original = self.preprocessor.scaler_y.inverse_transform(\n",
        "                y_test[:, -1].reshape(-1, 1)).flatten()\n",
        "\n",
        "            mae = mean_absolute_error(y_test_original, forecast)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test_original, forecast))\n",
        "\n",
        "            print(f\"Naive Forecast - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "            return forecast, mae, rmse\n",
        "\n",
        "    def mlp_baseline(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "        \"\"\"MLP baseline model\"\"\"\n",
        "        print(\"Training MLP baseline...\")\n",
        "\n",
        "        # Flatten the input sequences for MLP\n",
        "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "        X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
        "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "        # Only predict the last time step for baseline comparison\n",
        "        y_train_flat = y_train[:, -1]\n",
        "        y_val_flat = y_val[:, -1]\n",
        "        y_test_flat = y_test[:, -1]\n",
        "\n",
        "        # Build MLP model\n",
        "        model = tf.keras.Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(X_train_flat.shape[1],)),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1)  # Single output for the forecast horizon\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                     loss='mse', metrics=['mae'])\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(\n",
        "            X_train_flat, y_train_flat,\n",
        "            validation_data=(X_val_flat, y_val_flat),\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_flat)\n",
        "\n",
        "        # Inverse transform predictions\n",
        "        y_test_original = self.preprocessor.scaler_y.inverse_transform(\n",
        "            y_test_flat.reshape(-1, 1)).flatten()\n",
        "        y_pred_original = self.preprocessor.scaler_y.inverse_transform(y_pred).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = mean_absolute_error(y_test_original, y_pred_original)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
        "\n",
        "        print(f\"MLP Baseline - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "        return y_pred_original, mae, rmse, history, model\n",
        "\n",
        "# Train baseline models\n",
        "print(\"\\nTraining baseline models...\")\n",
        "baseline = BaselineModels(preprocessor)\n",
        "\n",
        "# SARIMA baseline\n",
        "sarima_forecast, sarima_mae, sarima_rmse = baseline.sarima_baseline(\n",
        "    y_train, y_test)\n",
        "\n",
        "# MLP baseline\n",
        "mlp_forecast, mlp_mae, mlp_rmse, mlp_history, mlp_model = baseline.mlp_baseline(\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbgfwVTPuIAv",
        "outputId": "edc7f3fa-c9a8-4189-f9fc-bb00d593c591"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training baseline models...\n",
            "Training SARIMA baseline...\n",
            "SARIMA - MAE: 30.2001, RMSE: 30.3471\n",
            "Training MLP baseline...\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "MLP Baseline - MAE: 8.7054, RMSE: 9.2174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"Custom attention layer for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weights\",\n",
        "                                shape=(input_shape[-1], self.units),\n",
        "                                initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\",\n",
        "                                shape=(self.units,),\n",
        "                                initializer=\"zeros\")\n",
        "        self.V = self.add_weight(name=\"att_v\",\n",
        "                                shape=(self.units, 1),\n",
        "                                initializer=\"normal\")\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate attention scores\n",
        "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
        "        attention_weights = tf.nn.softmax(tf.tensordot(score, self.V, axes=1), axis=1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        context_vector = attention_weights * inputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        ""
      ],
      "metadata": {
        "id": "FjJHYFoRub-q"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}